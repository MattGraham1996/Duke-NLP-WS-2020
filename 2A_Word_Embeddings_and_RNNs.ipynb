{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP) in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together. Let's play around with a set of pre-trained word vectors, to get used to their properties. There exist many sets of pretrained word embeddings; here, we use ConceptNet Numberbatch, which provides a relatively small download in an easy-to-work-with format (h5).\n",
    "\n",
    "To read an h5 file, we'll need to use the `h5py` package. \n",
    "If you followed the PyTorch installation instructions in 0A, you should have it downloaded already.\n",
    "Otherwise, you can install it with\n",
    "```Shell \n",
    "pip install h5py\n",
    "```\n",
    "You may need to re-open this notebook for the installation to take effect.\n",
    "\n",
    "Below, we use the package to open the `mini.h5` file we just downloaded. \n",
    "We extract from the file a list of utf-8-encoded words, as well as their 300-d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the file and pull out words and embeddings\n",
    "# import h5py\n",
    "\n",
    "# with h5py.File('datasets/mini.h5', 'r') as f:\n",
    "#     all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "#     all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"all_words dimensions: {}\".format())\n",
    "print(\"all_embeddings dimensions: {}\".format())\n",
    "\n",
    "print(\"Random example word: {}\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `all_words` is a list of $V$ strings (what we call our *vocabulary*), and `all_embeddings` is a $V \\times 300$ matrix. The strings are of the form `/c/language_code/word`â€”for example, `/c/en/cat` and `/c/es/gato`.\n",
    "\n",
    "We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character `/c/en/` prefix) and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict our vocabulary to just the English words\n",
    "\n",
    "\n",
    "print(\"word in all_words: {0}\".format())\n",
    "print(\"all_embeddings dimensions: {0}\".format())\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of a word vector is less important than its direction; the magnitude can be thought of as representing frequency of use, independent of the semantics of the word. \n",
    "Here, we will be interested in semantics, so we *normalize* our vectors, dividing each by its length. \n",
    "The result is that all of our word vectors are length 1, and as such, lie on a unit circle. \n",
    "The dot product of two vectors is proportional to the cosine of the angle between them, and provides a measure of similarity (the bigger the cosine, the smaller the angle).\n",
    "\n",
    "<img src=\"Figures/cosine_similarity.png\" alt=\"cosine\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look up words easily, so we create a dictionary that maps us from a word to its index in the word embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', )\n",
    "\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', )\n",
    "print('cat\\tdog\\t', )\n",
    "\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', )\n",
    "print('cat\\tfreeze\\t', )\n",
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonyms\\topposites\\t', )\n",
    "print('antonyms\\tsynonyms\\t', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find, for instance, the most similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `closest_to_vector` to find words \"nearby\" vectors that we create ourselves. This allows us to solve analogies. For example, in order to solve the analogy \"man : brother :: woman : ?\", we can compute a new vector `brother - man + woman`: the meaning of brother, minus the meaning of man, plus the meaning of woman. We can then ask which words are closest, in the embedding space, to that new vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three results are quite good, but in general, the results of these analogies can be disappointing. Try experimenting with other analogies, and see if you can think of ways to get around the problems you notice (i.e., modifications to the solve_analogy algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word embeddings in deep models\n",
    "\n",
    "Word embeddings are fun to play around with, but their primary use is that they allow us to think of words as existing in a continuous, Euclidean space; we can then use an existing arsenal of techniques for machine learning with continuous numerical data (like logistic regression or neural networks) to process text.\n",
    "\n",
    "Let's take a look at an especially simple version of this. \n",
    "We'll perform *sentiment analysis* on a set of movie reviews: in particular, we will attempt to classify a movie review as positive or negative based on its text.\n",
    "\n",
    "We will use a [Simple Word Embedding Model](http://people.ee.duke.edu/~lcarin/acl2018_swem.pdf) (SWEM, Shen et al. 2018) to do so. \n",
    "We will represent a review as the *mean* of the embeddings of the words in the review. \n",
    "Then we'll train a two-layer MLP (a neural network) to classify the review as positive or negative.\n",
    "As you might guess, using just the mean of the embeddings discards a lot of the information in a sentences, but for tasks like sentiment analysis, it can be surprisingly effective.\n",
    "\n",
    "If you don't have it already, download the `movie-simple.txt` file. \n",
    "Each line of that file contains \n",
    "\n",
    "1. the numeral 0 (for negative) or the numeral 1 (for positive), followed by\n",
    "2. a tab (the whitespace character), and then\n",
    "3. the review itself.\n",
    "\n",
    "Let's first read the data file, parsing each line into an input representation and its corresponding label.\n",
    "Again, since we're using SWEM, we're going to take the mean of the word embeddings for all the words as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "\n",
    "# Pull out the first character: that's our label (0 or 1)\n",
    "\n",
    "\n",
    "# Split the line into words using Python's split() function\n",
    "\n",
    "\n",
    "# Look up the embeddings of each word, ignoring words not\n",
    "# in our pretrained vocabulary.\n",
    "\n",
    "\n",
    "# Take the mean of the embeddings\n",
    "\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "\n",
    "\n",
    "# Concatenate all examples into a numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of inputs: {}\".format())\n",
    "print(\"Shape of labels: {}\".format())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've parsed the data, let's save 20\\% of the data (rounded to a whole number) for testing, using the rest for training.\n",
    "The file we loaded had all the negative reviews first, followed by all the positive reviews, so we need to shuffle it before we split it into the train and test splits.\n",
    "We'll then convert the data into PyTorch Tensors so we can feed them into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 10 labels before shuffling: {0}\".format())\n",
    "\n",
    "\n",
    "\n",
    "print(\"First 10 labels after shuffling: {0}\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could format each batch individually as we feed it into the model, but to make it easier on ourselves, let's create a TensorDataset and DataLoader as we've used in the past for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our model in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build the model, organized as a `nn.Module`.\n",
    "We could make the number of outputs for our MLP the number of classes for this dataset (2).\n",
    "However, since we only have two output classes here (\"positive\" vs \"negative\"), we can instead produce a single output value, calling everything greater than $0$ \"postive\" and everything less than $0$ \"negative\".\n",
    "If we pass this output through a sigmoid operation, then values are mapped to $[0,1]$, with $0.5$ being the classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we instantiate the model. \n",
    "Notice that since we use the binary cross-entropy (BCE) loss instead of the cross-entropy loss we've seen before.\n",
    "We use the \"with logits\" version for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "# Instantiate model\n",
    "\n",
    "\n",
    "# Binary cross-entropy (BCE) Loss and SGD Optimizer\n",
    "\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "\n",
    "# Zero out the gradients\n",
    "        \n",
    "# Forward pass\n",
    "\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "\n",
    "# Print training progress\n",
    "print(\"Epoch: {0} \\t Train Loss: {1} \\t Train Acc: {2}\".format())\n",
    "\n",
    "## Testing\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through test set minibatchs \n",
    "\n",
    "# Forward pass\n",
    "    \n",
    "print('Test accuracy: {}'.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine what our model has learned, seeing how it responds to word vectors for different words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some words\n",
    "\n",
    "\n",
    "print(\"Sentiment of the word '{0}': {1}\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some words of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In the context of deep learning, sequential data is commonly modeled with Recurrent Neural Networks (RNNs).\n",
    "As natural language can be viewed as a sequence of words, RNNs are commonly used for NLP.\n",
    "As with the fully connected and convolutional networks we've seen before, RNNs use combinations of linear and nonlinear transformations to project the input into higher level representations, and these representations can be stacked with additional layers.\n",
    "\n",
    "#### Sentences as sequences\n",
    "The key difference between sequential models and the previous models we've seen is the presence of a \"time\" dimension: words in a sentence (or paragraph, document) have an ordering to them that convey meaning:\n",
    "\n",
    "<img src=\"Figures/sentence_as_a_sequence.PNG\" alt=\"basic_RNN\" style=\"width: 300px;\"/>\n",
    "\n",
    "In the example sequence above, the word \"Recurrent\" is the $t=1$ word, which we denote $w_1$; similarly, \"neural\" is $w_2$, and so on.\n",
    "As the preceding sections have hopefully impressed upon you, it is often more advantageous to model words as embedding vectors $x_1, ..., x_T$, rather than one-hot vectors (which tokens $w_1,...w_T$ correspond to), so our first step is often to do an embedding table look-up for each input word.\n",
    "\n",
    "Note that in our previous sentiment analysis example, we just took the average embedding across time, treating the input as a \"bag-of-words.\"\n",
    "For simple problems, this can work surprisingly well, but as you might imagine, the ordering of words in a sentence is often important, and sometimes, we'd like to be able to model this temporal meaning as well.\n",
    "Enter RNNs.\n",
    "\n",
    "#### Review: Fully connected layer\n",
    "\n",
    "Before we introduce the RNN, let's first again revist the fully connected layer that we used in our logistic regression and multilayer perceptron examples, with a few changes in notation:\n",
    "\n",
    "\\begin{align*}\n",
    "h = f(x W + b)\n",
    "\\end{align*}\n",
    "\n",
    "Instead of calling the result of the fully connected layer $y$, we're going to call it $h$, for hidden state.\n",
    "The variable $y$ is usually reserved for the final layer of the neural network; since logistic regression was a single layer, using $y$ was fine. \n",
    "However, if we assume there is more than one layer, it is more common to refer to the intermediate representation as $h$.\n",
    "Note that we also use $f()$ to denote a nonlinear activation function.\n",
    "In the past, we've seen $f()$ as a $\\text{ReLU}$, but this could also be a $\\sigma()$ or $\\tanh()$ nonlinearity.\n",
    "Visualized:\n",
    "\n",
    "<img src=\"Figures/rnn_mlp.PNG\" width=\"175\"/>\n",
    "\n",
    "The key thing to notice here is that we project the input $x$ with a linear transformation (with $W$ and $b$), and then apply a nonlinearity to the output, giving us $h$.\n",
    "During training, our goal is to learn $W$ and $b$.\n",
    "\n",
    "#### A basic RNN\n",
    "\n",
    "Unlike the previous examples we've seen using fully connected layers, sequential data have multiple inputs $x_1, ..., x_T$, instead of a single $x$.\n",
    "We need to adapt our models accordingly for an RNN.\n",
    "While there are several variations, a common basic formulation for an RNN is the Elman RNN, which is as follows&ast;:\n",
    "\n",
    "\\begin{align}\n",
    "h_t = \\tanh((x_t W_x + b_x) + (h_{t-1} W_h + b_h))\n",
    "\\end{align}\n",
    "\n",
    "where $\\tanh()$ is the hyperbolic tangent, a nonlinear activation function.\n",
    "RNNs process words one at a time in sequence ($x_t$), producing a hidden state $h_t$ at every time step.\n",
    "The first half of the above equation should look familiar; as with the fully connected layer, we are linearly transforming each input $x_t$, and then applying a nonlinearity.\n",
    "Notice that we apply the same linear transformation ($W_x$, $b_x$) at every time step.\n",
    "The difference is that we also apply a separate linear transform ($W_h$, $b_h$) to the previous hidden state $h_{t-1}$ and add it to our projected input.\n",
    "This feedback is called a *recurrent* connection.\n",
    "\n",
    "These directed cycles in the RNN architecture gives them the ability to model temporal dynamics, making them particularly suited for modeling sequences (e.g. text).\n",
    "We can visualize an RNN layer as follows:\n",
    "\n",
    "<img src=\"Figures/rnn.PNG\" width=\"350\"/>\n",
    "\n",
    "We can unroll an RNN through time, making the sequential aspect of them more obvious:\n",
    "\n",
    "<img src=\"Figures/rnn_unrolled.PNG\" width=\"700\"/>\n",
    "\n",
    "You can think of these recurrent connections as allowing the model to consider previous hidden states of a sequence when calculating the hidden state for the current input.\n",
    "\n",
    "<font size=\"1\">&ast;Note: We don't actually need two separate biases $b_x$ and $b_h$, as you can combine both biases into a single learnable parameter $b$. \n",
    "However, writing it separately helps make it clear that we're performing a linear transformation on both $x_t$ and $h_{t-1}$.\n",
    "Speaking of combining variables, we can also express the above operation by concatenating $x_t$ and $h_{t-1}$ into a single vector $z_t$, and then performing a single matrix multiply $z_t W_z + b$, where $W_z$ is essentially $W_x$ and $W_h$ concatenated.\n",
    "Indeed this is how many \"official\" RNNs modules are implemented, as the reduction in the number of separate matrix multiply operations makes it computationally more effecient.\n",
    "These are implementation details though.</font>\n",
    "\n",
    "#### RNNs in PyTorch\n",
    "How would we implement an RNN in PyTorch? \n",
    "There are quite a few ways, but let's build the Elman RNN from scratch first, using the embeddings matrix from our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, import PyTorch first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have our inputs in word embedding form already, with dimension 300 as before. Although we usually group multiple examples into minibatches for efficiency, we'll use a minibatch size of 1 for this example for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RNN, we project both the input $x_t$ and the previous hidden state $h_{t-1}$ to some hidden dimension, which we're going to choose to be 128.\n",
    "To perform these operations, we're going to define some variables we're going to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For projecting the input\n",
    "\n",
    "\n",
    "# For projecting the previous state\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define a function for one time step of the RNN.\n",
    "This function take the current input $x_t$ and previous hidden state $h_{t-1}$, performs the linear transformations $x W_x + b_x$ and $h W_h + b_h$, and then a hyperbolic tangent nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step of our RNN is going to require feeding in an input (i.e. the word representation) and the previous hidden state (the summary of preceding sequence).\n",
    "Note that at the beginning of a sentence, we don't have a previous hidden state, so we initialize it to some value, for example all zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding for first word\n",
    "\n",
    "\n",
    "# Initialize hidden state to 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take one time step of the RNN, we call the function we wrote, passing in $x_1$ and $h_0$.\n",
    "In this case, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass of one RNN step for time step t=1\n",
    "\n",
    "\n",
    "print(\"Hidden state h1 dimensions: {0}\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `RNN_step` function again to get the next time step output from our RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding for second word\n",
    "\n",
    "\n",
    "# Forward pass of one RNN step for time step t=2\n",
    "\n",
    "\n",
    "print(\"Hidden state h2 dimensions: {0}\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue unrolling the RNN as far as we need to. \n",
    "For each step, we feed in the current input ($x_t$) and previous hidden state ($h_{t-1}$) to get a new output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, much like fully connected and convolutional layers, we typically don't implement RNNs from scratch as above, instead relying on higher level APIs.\n",
    "PyTorch has RNNs implemented in the `torch.nn` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"RNN parameter shapes: {}\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the RNN created by `torch.nn` produces parameters of the same dimensions as our from scratch example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gated RNNs\n",
    "\n",
    "While the RNNs we've just explored can successfully model simple sequential data, they tend to struggle with longer sequences, with [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) an especially big problem.\n",
    "A number of RNN variants have been proposed over the years to mitigate this issue and have been shown empirically to be more effective.\n",
    "In particular, Long Short-Term Memory (LSTM) and the Gated Recurrent Unit (GRU) have seen wide use recently in deep learning.\n",
    "We're not going to go into detail here about what structural differences they have from vanilla RNNs; a fantastic summary can be found [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "Note that \"RNN\" as a name is somewhat overloaded: it can refer to both the basic recurrent model we went over previously, or recurrent models in general (including LSTMs and GRUs).\n",
    "\n",
    "LSTMs and GRUs layers can be created in much the same way as basic RNN layers.\n",
    "Again, rather than implementing it yourself, it's recommend to use the `torch.nn` implementations, although we highly encourage that you peek at the source code so you understand what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"LSTM parameters: {}\".format())\n",
    "\n",
    "\n",
    "print(\"GRU parameters: {}\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext\n",
    "\n",
    "Much like PyTorch has [Torchvision](https://pytorch.org/docs/stable/torchvision/index.html) for computer vision, PyTorch also has [Torchtext](https://torchtext.readthedocs.io/en/latest/) for natural language processing.\n",
    "As with Torchvision, Torchtext has a number of popular NLP benchmark datasets, across a wide range of tasks (e.g. sentiment analysis, language modeling, machine translation).\n",
    "It also has a few pre-trained word embeddings available as well, including the popular Global Vectors for Word Representation (GloVe).\n",
    "If you need to load your own dataset, Torchtext has a number of useful containers that can make the data pipeline easier.\n",
    "\n",
    "At the moment, Torchtext isn't included with the PyTorch installation, and it isn't included in Anaconda or available for `pip`. \n",
    "This may change, but for the time being, if you'd like to take advantage of Torchtext, you can clone the [GitHub repo](https://github.com/pytorch/text) and follow their installation instructions there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other materials:\n",
    "Natural Language Processing can be several full courses on its own at most universities, both with or without neural networks.\n",
    "Here are some additional reads:\n",
    "\n",
    "- [Fantastic introduction to LSTMs and GRUs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Popular blog post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
